{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c869fd33",
   "metadata": {},
   "source": [
    "# UCL BIOC0016 - Bioinformatics\n",
    "Alan R. Lowe (a.lowe@ucl.ac.uk)\n",
    "\n",
    "---\n",
    "\n",
    "### Bioimage informatics / Machine learning\n",
    "\n",
    "The aim of the exercises in this notebook is to familiarise you with the different steps of evaluating the performance of a simple convolutional neural network (CNN). You will use a real CNN to classify different cell states using image data provided to you. A more sophisticated version of this neural network has been used in recently published research. The architecture of the network is similar to those you will have learnt about in the lectures.\n",
    "\n",
    "The network is able to tell whether a cell is proliferating or dead.  This could be useful to understand whether a drug is effective or not, or to understand the normal cellular behaviour.\n",
    "\n",
    "The CNN has been pre-trained on a dataset of real images from data collected at UCL. This is a very simple CNN, and you are going to assess the performance of it by making predictions of the cell state using image data, and comparing these with your ground truth annotation.  The images that you are provided with are called a 'hold out' set, since they have come from the much larger original data set, but have not been used to train the neural network. The CNN has not 'seen' these data during the training phase, and therefore represent a real test of the performance of the network.  \n",
    "\n",
    "The practical contains following sections:\n",
    "\n",
    "1. Data annotation\n",
    "2. Make predictions with a convolutional neural network\n",
    "3. Compare these with the data that you have annotated\n",
    "4. Determine the accuracy of the model\n",
    "\n",
    "### Instructions\n",
    "\n",
    "Code sections are coloured according to the following scheme:\n",
    "\n",
    "* <div class=\"task_red\"> Code that needs to be written by you. </div>\n",
    "* <div class=\"task_blue\"> Code that needs to be edited by you, perhaps changing some parameters. </div>\n",
    "* <div class=\"task_green\"> A task that needs to be completed by you. This may be recording the results. </div>\n",
    "\n",
    "\n",
    "### Be part of the research project!\n",
    "\n",
    "This is based on a real research project, and you can contribute to the project by recording your results as part of the practical. You can read more about the research project [here](http://lowe.cs.ucl.ac.uk/cellx.html).\n",
    "\n",
    "As you complete different sections of the practical, your annotations will be recorded anonymously. By recording these results, you are contributing to the research programme.\n",
    "\n",
    "\n",
    "### IMPORTANT NOTES:\n",
    "\n",
    "1. This notebook will 'timeout' if you do not interact with it for more than 10-20 minutes. Make sure you save your progress occasionally.\n",
    "2. If the colors above are not showing, you can run the first line of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59890b4e",
   "metadata": {},
   "source": [
    "---\n",
    "### Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e5f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bioimageml\n",
    "\n",
    "from bioimageml.api import validate_api_token\n",
    "from bioimageml.dataset import CellDataset\n",
    "from bioimageml.model import inspect, load_model, predict\n",
    "from bioimageml.visualise import visualize_confusion_matrix, visualize_random_batch, visualise_outputs, visualize_predictions, visualize_report\n",
    "from bioimageml.widgets import MitoticInstanceLabeller"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4eee0154",
   "metadata": {},
   "source": [
    "Now we need to provide a token to the API. It will be something like this:\n",
    "```python\n",
    "validate_api_token(\"6FxFD67hFDSMw\")\n",
    "```\n",
    "\n",
    "\n",
    "Please enter the token listed on the moodle page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fe81bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_api_token(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702daad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bioimageml._set_css_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fe72e3",
   "metadata": {},
   "source": [
    "---\n",
    "## PART 1 - Data Annotation\n",
    "\n",
    "\n",
    "To test how well the neural network is able to perform on unseen images, we need to manually annotate some new and unseen images with a label. We can then analyse how many labels the neural network correctly predicts. In this section, you will be provided with a random sample of unlabeled images. You will need at least 50, although the more the better. Using the guide below, please annotate each image with one of the six labels provided. If you are unsure, use the 'Unknown' label.\n",
    "\n",
    "Here are some examples of cells and their corresponding labels:\n",
    "\n",
    "![Cell_state_labels](./files/cell_states.png \"Cell state labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e2fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = CellDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404b8485",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_random_batch(d, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebeb321",
   "metadata": {},
   "source": [
    "<div class=\"task_blue\"> <b>TASK</b>: Try changing the number of images by changing the batch size.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071210fc",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Now we're going to start an in-browser widget to enable you to generate some testing labels. A new image will be shown and you can choose which label from those above that you think best represents the image.\n",
    "\n",
    "Every answer that you submit will be recorded and used to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ac87cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "widget = MitoticInstanceLabeller(d)\n",
    "widget.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a7008c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "widget.widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49983444",
   "metadata": {},
   "source": [
    "At any point you can see which annotations you've already made, using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddb9729",
   "metadata": {},
   "outputs": [],
   "source": [
    "widget.statistics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5eada4c3",
   "metadata": {},
   "source": [
    "When you think you have enough annotations, you can collect the annotations for further analysis, using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4291aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = widget.annotations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01870eb0",
   "metadata": {},
   "source": [
    "---\n",
    "## PART 2 - Using the CNN to make predictions\n",
    "\n",
    "### Loading the pre-trained model\n",
    "\n",
    "In the following lines of code `load_model()` builds the convolutional neural network (CNN) that we will test, and sets the weights and biases using the pretrained values. This model has been trained with thousands of images, but the network is very simple. The goal is to assess the performance of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4683ece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84e8c7cd",
   "metadata": {},
   "source": [
    "Now that you have loaded the model, try to work out the architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bb54a0",
   "metadata": {},
   "source": [
    "<div class='task_red'> <b>TASK:</b> Summarise the model and get the number of parameters. Answer the following questions:\n",
    "    <ul>\n",
    "        <li>How many convolutional layers are there? </li>\n",
    "        <li>How many kernels are used in each convolutional layer? </li>\n",
    "        <li>How many output classes are there? </li>\n",
    "        <li>What is the size of the input image, and after the convolutional layers?</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "**HINT**: You can use the command `model` to get the details of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507c9c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b071b4a",
   "metadata": {},
   "source": [
    "## Visualise activations within the network\n",
    "\n",
    "When the network is given an example, we can visualize the activations within the layes of the network from the sample. To visualize the activations within the network, you can use the `visualize_outputs()` command.\n",
    "\n",
    "To do this, the function needs three (3) arguments:\n",
    "```python\n",
    "inpsect(model, image, layer)\n",
    "```\n",
    "\n",
    "You can only use one image at a time, so you need to pass just one image as an argument. You could do this by selecting the first image (0) or the second (1), third (2) etc..:\n",
    "```python\n",
    "image = x[0]   # select the first image from the list\n",
    "```\n",
    "\n",
    "<br />\n",
    "\n",
    "<div class='task_red'><b>TASK:</b> visualize the activations within the network for the first, second and third convolutional layers (layer=1, 2 or 3). What can you see? </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570c0c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df7886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = inspect(model, [image], layer=1)\n",
    "visualise_outputs(activations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0913a3de",
   "metadata": {},
   "source": [
    "Note that the bright regions signify high activations, and the dark regions signify low activations. How many different images are there for one input at this level of the CNN?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b481e41d",
   "metadata": {},
   "source": [
    "### Make the predictions\n",
    "\n",
    "\n",
    "Now that we have loaded the pre-trained model. We can feed it the same images that you have labeled and generate predictions for the label. The predictions are returned as the probability of the label given the image data, $\\text{P}(\\text{label} | \\text{data})$. In fact, the model returns the probability for *all* labels given the data.\n",
    "\n",
    "<br />\n",
    "<div class=\"task_blue\"> <b>TASK:</b> Using the model, make predictions for the images that you have annotated.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65bfacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(model, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f8ecc4",
   "metadata": {},
   "source": [
    "Try looking at the predictions by typing `predictions`, what do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53f05ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750ac6e9",
   "metadata": {},
   "source": [
    "---\n",
    "## PART 3 - Compare the predictions with your annotations\n",
    "\n",
    "We can visualize the images and their predictions using one of the helper functions:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "342740b6",
   "metadata": {},
   "source": [
    "<div class='task_blue'><b>TASK:</b> Print out the predictions from the model using the helper functions below. How well do they compare to the annotations you have made? Which predictions is the model confident in, and which ones are less confident? </div>\n",
    "\n",
    "**HINT:** You can use the following helper functions to do this:\n",
    "`visualize_predictions(x, predictions)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd2614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(x, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0ba863",
   "metadata": {},
   "source": [
    "### Calculate the confusion matrix\n",
    "\n",
    "A *confusion matrix* is used to assess how well a multi-label classifier performs. The matrix is square; it has the same number of rows and columns, $n \\times n$, where $n$ is the number of classes.\n",
    "\n",
    "By definition, a confusion matrix $\\mathbf{C}$ is such that $\\mathbf{C}_{i,j}$ is equal to the number of observations known to be in group $i$ and predicted to be in group $j$.\n",
    "\n",
    "If the predictions for every label were correct, i.e. all predictions equal the ground truth, then all of the entries will be on the diagonal of the matrix:\n",
    "\n",
    "![Confusion_matrix](./files/confusion.png \"Confusion matrix\")\n",
    "\n",
    "<div class='task_blue'><b>TASK: </b> Use the helper function `calculate_confusion_matrix` to create the confusion matrix. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40780f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_confusion_matrix(x, predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a9739e9",
   "metadata": {},
   "source": [
    "Which predictions are good? Which are bad?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edde9fd3",
   "metadata": {},
   "source": [
    "---\n",
    "## PART 4 - Calculate the accuracy of the model using your ground truth data\n",
    "\n",
    "We need to determine the overall accuracy of the model. We will make several simple calculations:\n",
    "\n",
    "\n",
    "1. **Accuracy** is a single number that captures the fraction of true predictions among the total number of examples examined.\n",
    "2. **Precision** is the fraction of correct label $i$ out of all instances where the model predicted a label $i$.  Precision is a good metric when we want to be very sure that our model is making correct predictions.\n",
    "3. **Recall** is the fraction of correct label $i$ out of all instances where the true label is $i$. Recall is a good metric when we want our model to capture as many positives predictions as possible.\n",
    "\n",
    "It's important to think about all of these metrics. Ideally we would want our model to correctly predict the label (precision) and to label all occurences (recall)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50bb6cf9",
   "metadata": {},
   "source": [
    "#### F1-score\n",
    "\n",
    "The final metric we will calculate is called the $F_1$-score. This metric tries to balance the precision and recall into a single score, for each label of the classifier.  This is important to enable us to assess the performance of the model and determine whether it works well enough for our purposes, or to compare different models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a3b5a76",
   "metadata": {},
   "source": [
    "The $F_{1}$ score can be calculated according to the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "F_1 = 2 \\times \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}}\n",
    "\\end{equation}\n",
    "\n",
    "<br />\n",
    "\n",
    "<div class='task_red'><b>TASK:</b> Use the function below to calculate the F1 score for each class </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184cc67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_report(x, predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4dac0e86",
   "metadata": {},
   "source": [
    "Is this a good model? What are the challenges? How would you change things to get a better result?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3d92ef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# End of practical"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfeaaef6",
   "metadata": {},
   "source": [
    "### Further reading\n",
    "\n",
    "Here are some publications that use these data:\n",
    "\n",
    "**Learning biophysical determinants of cell fate using deep neural networks**  \n",
    "Soelistyo C, Vallardi G, Charras G and Lowe AR.  \n",
    "*Nature Machine Intelligence* (2022)  \n",
    "[![doi:10.1038/s42256-022-00503-6](https://img.shields.io/badge/doi-10.1038%2Fs42256--022--00503--6-blue)](https://doi.org/10.1038/s42256-022-00503-6)  \n",
    "\n",
    "\n",
    "**Automated deep lineage tree analysis using a Bayesian single cell tracking approach**  \n",
    "Ulicna K, Vallardi G, Charras G and Lowe AR.  \n",
    "*Frontiers in Computer Science* (2021)  \n",
    "[![doi:10.3389/fcomp.2021.734559](https://img.shields.io/badge/doi-10.3389%2Ffcomp.2021.734559-blue)](https://doi.org/10.3389/fcomp.2021.734559)\n",
    "\n",
    "\n",
    "**Local cellular neighbourhood controls proliferation in cell competition**  \n",
    "Bove A, Gradeci D, Fujita Y, Banerjee S, Charras G and Lowe AR.  \n",
    "*Molecular Biology of the Cell* (2017)  \n",
    "[![doi:10.1091/mbc.E17-06-0368](https://img.shields.io/badge/doi-10.1091%2Fmbc.E17--06--0368-blue)](https://doi.org/10.1091/mbc.E17-06-0368)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0201577f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "85ebd2266c4bec227b74f6e1765cd42ed28d610f5e56c414e57bbf0cbc9745b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
